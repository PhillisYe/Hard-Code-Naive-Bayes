{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPSCI 762 Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this programming project is to use a Naive Bayes algorithm to achieve high predictive accuracy on the test data. The package I used in this project is only `numpy`. The first step used `numpy` to load in the training data \"trg.csv\" and I checked there are no missing values in this data set, and all the abstracts are at lower case letters and no punctuation, so there is no need to do the preprocessing with the data. Linking to the Naive Bayes algorithm, the probability of an abstract in a class is proportional to the probability of that class multiplied by the conditional probabilities of each word that appeared in that abstract if that word is in the count. Therefore, I tried to obtain an attribute probability representation of each word in each class. The main idea is to split each 'abstract' into words and calculate the conditional probabilities of each word that would appear in each 'class' and the probability of 'classes'.\n",
    "\n",
    "For the model building, I first calculated the probabilities of each class and noticed that *class B* and *class C* take the majority of all the class labels. Therefore, I build a null model by randomly choose the class label from *class B* and *class C* as the predicted value and expect to have an accuracy of around 50%. \n",
    "\n",
    "For the running time efficiency, I first chose the 1000 words with the most occurrence frequency in the data. Then I write a standard naive base algorithm to multiple all the probabilities together. Based on the cases that there would be zero counts of a word in an abstract, which would lead to a zero conditional probability and results in a zero Naive Bayes probability; and multiplication of a lot of probabilities may lead to a minimal value with a lot of zero digits, which may cause some problems in the final answers. Therefore, I write an improved naive base algorithm that calculates conditional probability by adding a Laplace smoothing of 1 to the denominator and the number of unique words to the numerator and then using log() to add up the probabilities. Finally, in order to place less weight on very common words and more weight on less common word, I used the TF-IDF to replace the word frequency value with the multiplication of the word frequency and the log of the inverse document frequency to extend the improved naive base. Therefore, I have a total of four models to test the performance.\n",
    "\n",
    "A cross-validation function splits the training data \"trgarr\" into a training set and a validation set to test the performance of the different model and different parameters. The training set is used to choose the words and calculate the conditional probability of each word in each class and the class probability. Then the validation set is used to predict the class for each abstract based on the algorithm built from the training set and validate the accuracy with the actual class. I chose the 5-folds cross-validation for time efficiency. Firstly, I used 5-folds cross-validation to test the training set without deleting the stop words. I ran cross-validation for the null model and the standard naive Bayes model with 1000 best words and the standard naive Bayes model with all the words. The accuracy score of the null model is 47.3% which means I can always have at least around 47.3% accuracy on the prediction, and there can be further improvement by using the naive Bayes algorithm. As the results shown in Table 1, the accuracy score of the standard naive Bayes dropped rapidly from 0.899 to 0.2045 by using all the words, it would be because many words do not occur in the abstract and lead to a zero probability, which affects the prediction. Thus it is important to add the Laplace smoothing and using log() to add up the probabilities. Then I removed the stop words such as \"the\", \"of\", which are useless and run the cross-validation for the standard naive Bayes algorithm, improved naive Bayes algorithm with smoothing and improved naive Bayes with TF-IDF with 1000 best words and the improved naive Bayes with TF-IDF with 15000 words. As shown in Table 2, the accuracy score of the standard naive Bayes has increased by 0.03, which means removing the stop words helped a lot to the prediction. The accuracy scores of the improved naive Bayes with smoothing and with TF-IDF are around 90% which are lower than the standard naive Bayes. However, the standard naive Bayes is sensitive to the choice of the number of words as it will drop down rapidly if the number of best words increases. As a result, for the improved naive Bayes with TF-IDF with 15000 best words, the accuracy score increases rapidly with about 5% accuracy. The number of the best words should also be a parameter that would affect the prediction accuracy. Then I tried to use cross-validation to choose the number of best words that result in the highest accuracy. Considering the running time, I ran the cross-validation with 5000 to 15000 number of best words with step size 1000. Then I find the **10000** number of best words give the highest accuracy score, and then the accuracy score goes down. (The results are in Table 3)\n",
    "\n",
    "I also tested the accuracy score on the whole training data for the three naive Bayes models with the results shown in Table 4. By choosing 10000 best words, the accuracy scores for the improved naive Bayes with smoothing and the improved naive Bayes with TF-IDF are similar. Therefore, I tried both the improved naive Bayes without TF-IDF and the improved naive Bayes with TF-IDF with 10000 best words to train the whole training data \"trg.csv\" and apply the probabilities and algorithm to predict the test data \"tst.csv\". Then I uploaded them onto Kaggle and got scores of 0.9533 for both of the algorithms. I think this is because the main issue of this training set is the class imbalance, but TF-IDF is not a method to deal with it and leads to no significant improvement on the test data. The naive Bayes model can be further extended by counting a class weight.\n",
    "\n",
    "  \n",
    "\n",
    "| Table 1    | Null Model | Standard NB | Standard NB |\n",
    "| :--------: | :--------: | :---------: | :---------: |\n",
    "|no. of words|    1000    |     1000    |     All     |\n",
    "| score      |   0.4728   |  **0.8997** |    0.2045   |\n",
    "\n",
    "\n",
    "|   Table 2  | Standard NB | INB with Smoothing | INB with TF-IDF | INB with TF-IDF |\n",
    "| :--------: | :---------: | :------------------------: | :---------------------: | :---------------------: |\n",
    "|no. of words|     1000    |            1000            |           1000          |           15000         |   \n",
    "| score      |    0.9208   |           0.9048           |          0.90025        |        **0.9518**       |\n",
    "\n",
    "\n",
    "Table 3\n",
    "\n",
    "|no. of words|  5000  |  6000  |  7000  |  8000  |  9000  |   10000  |  11000 |  12000 |  13000 |  14000 |  15000 |\n",
    "| :--------: | :----: | :----: | :----: | :----: | :----: | :------: | :----: | :----: | :----: | :----: | :----: | \n",
    "| score      | 0.9445 | 0.9468 | 0.9483 | 0.9497 | 0.9528 |**0.9533**| 0.9530 | 0.9520 | 0.9520 | 0.9515 | 0.9518 |\n",
    "\n",
    "\n",
    "|  Table 4   | Standard NB | Improved NB with Smoothing | Improved NB with TF-IDF |\n",
    "| :--------: | :---------: | :------------------------: | :---------------------: |\n",
    "|no. of words|    10000    |            10000           |           10000         |\n",
    "| score      |    0.5895   |          **0.983**         |          0.98075        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count occurrence frequency of all words in a text\n",
    "def count_word(d,text):\n",
    "    for word in text[1:-1].split(): #remove the \"\" at the begining and end of the string and split the text into separate words\n",
    "        if word.isdigit() == False: # remove integer values in the abstract\n",
    "            if len(word) >1: # remove words with length less than 1\n",
    "                if word not in d.keys(): #or word+'s' not in word_dict.keys() or word+'es' not in word_dict.keys():\n",
    "                    d[word] = 1\n",
    "                else:\n",
    "                    d[word] += 1\n",
    "    return d\n",
    "\n",
    "# remove the stop words and sort the words with descending occurrence frequency\n",
    "def sorted_words(abstracts):\n",
    "    word_dict = {}\n",
    "    for text in abstracts:\n",
    "        # building a dictionary to store words appear in the abstract\n",
    "        word_dict=count_word(word_dict,text)\n",
    "    \n",
    "    # sort the words occur frequency into descending order\n",
    "    sorted_dict = dict(sorted(word_dict.items(), key=lambda item:item[1], reverse=True))\n",
    "    return sorted_dict\n",
    "\n",
    "def remove_stop_words(d):\n",
    "    stop_words = ['the','of','and','in','to','is','for','are','was','by','as','an','we','has','be','been','that',\n",
    "                  'other','on','at','or','not','also','its','between','contains','but','all','it','with','from',\n",
    "                  'both','may','most','than','their','were','this','which','have','these','only','known','more',\n",
    "                  'here','using','those','into','no','many','high','within','large','each','there','however','approximately',\n",
    "                 'some','can','such','our','although','one','two','three','four','well','during','had','about','any',\n",
    "                 'they','show','shows','shown','thus','five','could','when','same','very','while','whereas','ii','iii',\n",
    "                 'under','so','over','almost','being','after','whole','then','even','yet','via','do','will','six',\n",
    "                  'seven','eight','nine','ten','iv','vi','upon','did','therefore','whose','them','having','able','does',\n",
    "                 'still']\n",
    "\n",
    "    for word in stop_words:\n",
    "        if word in d:\n",
    "            del d[word]\n",
    "    \n",
    "    return d\n",
    "\n",
    "# count the words occurrence frequency in each class\n",
    "def words_frequency(words, abstracts, labels):\n",
    "#def conditional_prob (words, abstracts, labels):\n",
    "    # initialise dictionaries for each of four domains\n",
    "    A_dict = {}\n",
    "    B_dict = {}\n",
    "    E_dict = {}\n",
    "    V_dict = {}\n",
    "\n",
    "    for word in words:\n",
    "        A_dict[word] = 0\n",
    "        B_dict[word] = 0\n",
    "        E_dict[word] = 0\n",
    "        V_dict[word] = 0\n",
    "\n",
    "    # count the words' occurrence frequency in the abstract to each domain.\n",
    "    for i in range(len(abstracts)):\n",
    "        for word in abstracts[i][1:-1].split():\n",
    "            if word in words:\n",
    "                #occurence frequency of each word in class A.\n",
    "                if labels[i] == 'A':\n",
    "                    A_dict[word] += 1\n",
    "                #occurence frequency of each word in class B.\n",
    "                if labels[i] == 'B':\n",
    "                    B_dict[word] += 1\n",
    "                #occurence frequency of each word in class E.\n",
    "                if labels[i] == 'E':\n",
    "                    E_dict[word] += 1\n",
    "                #occurence frequency of each word in class V.\n",
    "                if labels[i] == 'V':\n",
    "                    V_dict[word] += 1\n",
    "\n",
    "    return [A_dict,B_dict,E_dict,V_dict]\n",
    "\n",
    "def probability (d):\n",
    "    total = sum(d.values()) # total word occurrence frequency\n",
    "    unique = len(d) # number of unique words\n",
    "    for key, value in d.items():\n",
    "        # laplace smoothing\n",
    "        d[key] = np.log((value + 1)/(total + unique))\n",
    "    return d\n",
    "\n",
    "# calculate the conditional probability\n",
    "def conditional_prob(words, abstracts, labels):\n",
    "    frequency = words_frequency(words, abstracts, labels)\n",
    "    A = probability(frequency[0])\n",
    "    B = probability(frequency[1])\n",
    "    E = probability(frequency[2])\n",
    "    V = probability(frequency[3])\n",
    "    \n",
    "    return [A,B,E,V]\n",
    "\n",
    "# calculate the class probability\n",
    "def class_prob (label):\n",
    "    A_class = sum(label=='A') # occurence of class A\n",
    "    B_class = sum(label=='B') # occurence of class B\n",
    "    E_class = sum(label=='E') # occurence of class E\n",
    "    V_class = sum(label=='V') # occurence of class V\n",
    "    total = A_class + B_class + E_class + V_class\n",
    "\n",
    "    # probability of each class\n",
    "    return [np.log(A_class/total), np.log(B_class/total), np.log(E_class/total), np.log(V_class/total)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null model\n",
    "def null_model(X):\n",
    "    result = []\n",
    "    for i in range(len(X)):\n",
    "        predicted = np.random.choice(['B','E'])\n",
    "        result.append(predicted)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_probability(d):\n",
    "    total = sum(d.values()) # total word occurrence frequency\n",
    "    unique = len(d) # number of unique words\n",
    "    for key, value in d.items():\n",
    "        d[key] = value/total\n",
    "    return d\n",
    "\n",
    "# calculate the conditional probability\n",
    "def standard_cond_prob(words, abstracts, labels):\n",
    "    frequency = words_frequency(words, abstracts, labels)\n",
    "    A = standard_probability(frequency[0])\n",
    "    B = standard_probability(frequency[1])\n",
    "    E = standard_probability(frequency[2])\n",
    "    V = standard_probability(frequency[3])\n",
    "    \n",
    "    return [A,B,E,V]\n",
    "\n",
    "# calculate the class probability\n",
    "def standard_class_prob (label):\n",
    "    A_class = sum(label=='A') # occurence of class A\n",
    "    B_class = sum(label=='B') # occurence of class B\n",
    "    E_class = sum(label=='E') # occurence of class E\n",
    "    V_class = sum(label=='V') # occurence of class V\n",
    "    total = A_class + B_class + E_class + V_class\n",
    "\n",
    "    # probability of each class\n",
    "    return [A_class/total, B_class/total, E_class/total, V_class/total]\n",
    "\n",
    "def standard_predict(abstracts,cond_prob,class_prob,type_words):\n",
    "    result=[]\n",
    "    \n",
    "    # get the class probability\n",
    "    p_A = class_prob[0]\n",
    "    p_B = class_prob[1]\n",
    "    p_E = class_prob[2]\n",
    "    p_V = class_prob[3]\n",
    "    \n",
    "    # get the conditional probability\n",
    "    A_dict = cond_prob[0]\n",
    "    B_dict = cond_prob[1]\n",
    "    E_dict = cond_prob[2]\n",
    "    V_dict = cond_prob[3]\n",
    "    \n",
    "    for text in abstracts:\n",
    "        score_dict={}\n",
    "        score_dict['A']=p_A\n",
    "        score_dict['B']=p_B\n",
    "        score_dict['E']=p_E\n",
    "        score_dict['V']=p_V\n",
    "        \n",
    "        d = {}\n",
    "        counts = count_word(d,text)\n",
    "        \n",
    "        # naive bayes algorithm with log value\n",
    "        for word in counts.keys():\n",
    "            if word in type_words:\n",
    "                score_dict['A'] *= A_dict[word]\n",
    "                score_dict['B'] *= B_dict[word]\n",
    "                score_dict['E'] *= E_dict[word]\n",
    "                score_dict['V'] *= V_dict[word]\n",
    "        \n",
    "        value = list(score_dict.values())\n",
    "        # find the maximum probability is the predicted class\n",
    "        i = value.index(max(value))\n",
    "        key = list(score_dict.keys())\n",
    "        res = key[i]\n",
    "        result.append(res)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_abstract (words, abstracts):\n",
    "    d = {}\n",
    "    for word in words:\n",
    "        d[word] = 0\n",
    "        for text in abstracts[1:-1]:\n",
    "            if text.find(word):\n",
    "                d[word] += 1\n",
    "    return d\n",
    "\n",
    "# calculate the idf\n",
    "def idf(words, abstracts, labels):\n",
    "    length = len(abstracts)\n",
    "    word_dict = count_abstract(words,abstracts)\n",
    "\n",
    "    for i in word_dict.keys():\n",
    "        word_dict[i] = np.log(length/word_dict[i]+1)   #+1\n",
    "    return word_dict\n",
    "\n",
    "# calculate the TF-IDF value for each word in each class\n",
    "def tfidf(words, abstracts, labels):\n",
    "    tf = words_frequency(words, abstracts, labels)\n",
    "    idfd = idf(words, abstracts, labels)\n",
    "    \n",
    "    A = tf[0]\n",
    "    B = tf[1]\n",
    "    E = tf[2]\n",
    "    V = tf[3]\n",
    "    \n",
    "    A_tfidf = {}\n",
    "    B_tfidf = {}\n",
    "    E_tfidf = {}\n",
    "    V_tfidf = {}\n",
    "    \n",
    "    # replace frequency with frequency * IDF\n",
    "    for word in idfd:\n",
    "        A_tfidf[word] = A[word]*idfd[word]\n",
    "        B_tfidf[word] = B[word]*idfd[word]\n",
    "        E_tfidf[word] = E[word]*idfd[word]\n",
    "        V_tfidf[word] = V[word]*idfd[word]\n",
    "    \n",
    "    A_prob = probability(A_tfidf)\n",
    "    B_prob = probability(B_tfidf)\n",
    "    E_prob = probability(E_tfidf)\n",
    "    V_prob = probability(V_tfidf)\n",
    "    \n",
    "    return [A_prob,B_prob,E_prob,V_prob]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(abstracts,cond_prob,class_prob,type_words):\n",
    "    result=[]\n",
    "    \n",
    "    # get the class probability\n",
    "    p_A = class_prob[0]\n",
    "    p_B = class_prob[1]\n",
    "    p_E = class_prob[2]\n",
    "    p_V = class_prob[3]\n",
    "    \n",
    "    # get the conditional probability\n",
    "    A_dict = cond_prob[0]\n",
    "    B_dict = cond_prob[1]\n",
    "    E_dict = cond_prob[2]\n",
    "    V_dict = cond_prob[3]\n",
    "    \n",
    "    for text in abstracts:\n",
    "        score_dict={}\n",
    "        score_dict['A']=p_A\n",
    "        score_dict['B']=p_B\n",
    "        score_dict['E']=p_E\n",
    "        score_dict['V']=p_V\n",
    "        \n",
    "        words = text[1:-1].split()\n",
    "    \n",
    "        # naive bayes algorithm with log value\n",
    "        for word in words:\n",
    "            if word in type_words:\n",
    "                score_dict['A'] += A_dict[word]\n",
    "                score_dict['B'] += B_dict[word]\n",
    "                score_dict['E'] += E_dict[word]\n",
    "                score_dict['V'] += V_dict[word]\n",
    "        \n",
    "        value = list(score_dict.values())\n",
    "        # find the maximum probability is the predicted class\n",
    "        i = value.index(max(value))\n",
    "        key = list(score_dict.keys())\n",
    "        res = key[i]\n",
    "        result.append(res)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def standard_algorithm(Xtrain,Xvalidation,Ytrain,type_words):\n",
    "    cond_prob = standard_cond_prob(type_words, Xtrain, Ytrain)\n",
    "    label_prob = standard_class_prob(Ytrain)\n",
    "    \n",
    "    predicted = standard_predict(Xvalidation,cond_prob,label_prob,type_words)\n",
    "    return predicted\n",
    "                       \n",
    "def algorithm(Xtrain,Xvalidation,Ytrain,type_words):\n",
    "    cond_prob = conditional_prob(type_words, Xtrain, Ytrain)\n",
    "    label_prob = class_prob(Ytrain)\n",
    "    \n",
    "    predicted = predict(Xvalidation,cond_prob,label_prob,type_words)\n",
    "    return predicted\n",
    "\n",
    "def tfidf_algorithm(Xtrain,Xvalidation,Ytrain,type_words):\n",
    "    cond_prob = tfidf(type_words, Xtrain, Ytrain)\n",
    "    label_prob = class_prob(Ytrain)\n",
    "    \n",
    "    predicted = predict(Xvalidation,cond_prob,label_prob,type_words)\n",
    "    return predicted\n",
    "\n",
    "def accuracy(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct/len(actual)\n",
    "\n",
    "def cross_validation(data,n,type_words,model):\n",
    "    fold_size = int(len(data)/n) # number of rows in each fold\n",
    "    ntrain = int(fold_size*(n-1)) # number of rows in training set\n",
    "    ntest = int(fold_size) # number of rows in testing set\n",
    "    scores = []\n",
    "    for i in range (n):\n",
    "        start = fold_size * i\n",
    "        \n",
    "        # train test data split for X value\n",
    "        Xvalidation = data['abstract'][start:start+ntest]\n",
    "        Xtrain = np.append(data['abstract'][:start],data['abstract'][start+ntest:])\n",
    "        \n",
    "        # train test data split for Y value\n",
    "        Yvalidation = data['class'][start:start+ntest] \n",
    "        Ytrain = np.append(data['class'][:start],data['class'][start+ntest:])\n",
    "        \n",
    "        if model == 0:\n",
    "            predicted = null_model(Xtrain)\n",
    "        elif model == 1:\n",
    "            predicted = standard_algorithm(Xtrain,Xvalidation,Ytrain,type_words)\n",
    "        elif model == 2:\n",
    "            predicted = algorithm(Xtrain,Xvalidation,Ytrain,type_words)\n",
    "        elif model == 3:\n",
    "            predicted = tfidf_algorithm(Xtrain,Xvalidation,Ytrain,type_words)\n",
    "        \n",
    "        score = accuracy(Yvalidation, predicted)\n",
    "        scores.append(score)\n",
    "    return scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of missing value in the training set is: 0\n",
      "probability of class A:  0.032\n",
      "probability of class B:  0.4005\n",
      "probability of class E:  0.536\n",
      "probability of class V:  0.0315\n"
     ]
    }
   ],
   "source": [
    "# setting the data type of each column\n",
    "dt = np.dtype([('id', np.int32), ('class', 'object'), ('abstract', 'object')])\n",
    "\n",
    "# using numpy to read the trg.csv in as array\n",
    "trgarr = np.loadtxt(\"trg.csv\",delimiter=\",\", skiprows=1, dtype=dt)\n",
    "\n",
    "# check any missing value in the trgarr\n",
    "lis = [item != item for item in trgarr]\n",
    "print(\"number of missing value in the training set is:\", sum(lis))\n",
    "\n",
    "# look at the probability of each class occurs\n",
    "label = standard_class_prob(trgarr['class'])\n",
    "print(\"probability of class A: \", label[0])\n",
    "print(\"probability of class B: \", label[1])\n",
    "print(\"probability of class E: \", label[2])\n",
    "print(\"probability of class V: \", label[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words not deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = sorted_words(trgarr['abstract'])\n",
    "\n",
    "# list of all words\n",
    "all_words = list(sorted_dict.keys())\n",
    "# list of the 1000 most frequent occured words\n",
    "freq_words = all_words[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using cross validation to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for the null model is: 0.47275\n",
      "The accuracy score for using the 1000 best words: 0.8997499999999998\n",
      "The accuracy score for using all words: 0.2045\n"
     ]
    }
   ],
   "source": [
    "# The null model performance with 1000 best words\n",
    "null = cross_validation(trgarr,5,freq_words,0)\n",
    "print(\"The accuracy score for the null model is:\", sum(null)/len(null))\n",
    "\n",
    "# The standard naive bayes performance with 1000 best words\n",
    "score = cross_validation(trgarr,5,freq_words,1)\n",
    "print(\"The accuracy score for using the 1000 best words:\", sum(score)/len(score))\n",
    "\n",
    "# The standard naive bayes performance with all words\n",
    "score = cross_validation(trgarr,5,all_words,1)\n",
    "print(\"The accuracy score for using all words:\", sum(score)/len(score)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words = list(remove_stop_words(sorted_dict).keys())\n",
    "freq_clean = clean_words[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using cross validation to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Naive Bayes with 1000 best words: 0.92075\n",
      "Improved Naive Bayes with 1000 best words: 0.9047500000000002\n",
      "Improved Naive Bayes with TF-IDF with 1000 best words: 0.90025\n",
      "Improved Naive Bayes with TF-IDF with 15000 best words: 0.95175\n"
     ]
    }
   ],
   "source": [
    "# The standard naive bayes performance with 1000 best words\n",
    "score = cross_validation(trgarr,5,freq_clean,1)\n",
    "print(\"Standard Naive Bayes with 1000 best words:\", sum(score)/len(score))\n",
    "\n",
    "# The improved naive bayes performance with 1000 best words\n",
    "score = cross_validation(trgarr,5,freq_clean,2)\n",
    "print(\"Improved Naive Bayes with 1000 best words:\", sum(score)/len(score))\n",
    "\n",
    "# The improved naive bayes with TF-IDF performance with 1000 best words\n",
    "score = cross_validation(trgarr,5,freq_clean,3)\n",
    "print(\"Improved Naive Bayes with TF-IDF with 1000 best words:\", sum(score)/len(score))\n",
    "\n",
    "# The improved naive bayes with TF-IDF performance with all words\n",
    "score = cross_validation(trgarr,5,clean_words[:15000],3)\n",
    "print(\"Improved Naive Bayes with TF-IDF with 15000 best words:\", sum(score)/len(score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 0.9445\n",
      "6000 0.9467500000000001\n",
      "7000 0.94825\n",
      "8000 0.9497499999999999\n",
      "9000 0.95275\n",
      "10000 0.95325\n",
      "11000 0.9530000000000001\n",
      "12000 0.952\n",
      "13000 0.952\n",
      "14000 0.9515\n",
      "15000 0.95175\n"
     ]
    }
   ],
   "source": [
    "# Test the accuracy score of naive bayes with TF-IDF performance with 5000 to 15000 best words with step size 1000\n",
    "for i in range (5000,15001,1000):\n",
    "    word_length = clean_words[:i]\n",
    "    score = cross_validation(trgarr,5,word_length,3)\n",
    "    print(i, sum(score)/len(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Naive Bayes with 10000 best words on full training set: 0.5895\n",
      "Improved Naive Bayes with 10000 best words on full training set: 0.983\n",
      "Improved Naive Bayes with TF-IDF with 10000 best words on full training set: 0.98075\n"
     ]
    }
   ],
   "source": [
    "# accuracy score on full training set by using standard naive bayes with 10000 best words\n",
    "cond_prob = standard_cond_prob(clean_words[:10000], trgarr['abstract'], trgarr['class'])\n",
    "label_prob = standard_class_prob(trgarr['class'])\n",
    "predicted = predict(trgarr['abstract'],cond_prob,label_prob,clean_words[:10000])\n",
    "score = accuracy(trgarr['class'], predicted)\n",
    "print(\"Standard Naive Bayes with 10000 best words on full training set:\", score)\n",
    "\n",
    "# accuracy score on full training set by using improved naive bayes with 1000 best words\n",
    "cond_prob = conditional_prob(clean_words[:10000], trgarr['abstract'], trgarr['class'])\n",
    "label_prob = class_prob(trgarr['class'])\n",
    "predicted = predict(trgarr['abstract'],cond_prob,label_prob,clean_words[:10000])\n",
    "score = accuracy(trgarr['class'], predicted)\n",
    "print(\"Improved Naive Bayes with 10000 best words on full training set:\", score)\n",
    "\n",
    "# accuracy score on full training set by using improved naive bayes with TF-IDF with 1000 best words\n",
    "cond_prob = tfidf(clean_words[:10000], trgarr['abstract'], trgarr['class'])\n",
    "label_prob = class_prob(trgarr['class'])\n",
    "predicted = predict(trgarr['abstract'],cond_prob,label_prob,clean_words[:10000])\n",
    "score = accuracy(trgarr['class'], predicted)\n",
    "print(\"Improved Naive Bayes with TF-IDF with 10000 best words on full training set:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the data type of each column\n",
    "dt = np.dtype([('id', np.int32), ('abstract', 'object')])\n",
    "\n",
    "# using numpy to read the test data in as array\n",
    "tstarr = np.loadtxt(\"tst.csv\",delimiter=\",\", skiprows=1, dtype=dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting for the Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = tstarr['abstract']\n",
    "words_10000 = clean_words[:10000]\n",
    "\n",
    "# without TF-IDF\n",
    "cond_prob = conditional_prob(words_10000, trgarr['abstract'],trgarr['class'])\n",
    "label_prob = class_prob(trgarr['class'])\n",
    "    \n",
    "result1 = predict(abstract,cond_prob,label_prob,words_10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with TF-IDF\n",
    "cond_prob = tfidf(words_10000,trgarr['abstract'],trgarr['class'])\n",
    "label_prob = class_prob(trgarr['class'])\n",
    "\n",
    "result2 = predict(abstract,cond_prob,label_prob,words_10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the output to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 'B']\n",
      " [2 'E']\n",
      " [3 'E']\n",
      " ...\n",
      " [998 'E']\n",
      " [999 'A']\n",
      " [1000 'E']]\n",
      "[[1 'B']\n",
      " [2 'E']\n",
      " [3 'E']\n",
      " ...\n",
      " [998 'E']\n",
      " [999 'A']\n",
      " [1000 'E']]\n"
     ]
    }
   ],
   "source": [
    "# without TF-IDF\n",
    "empty_array1=np.empty((1000,0),object)\n",
    "\n",
    "comb_arr1 = np.append(empty_array1,np.array([tstarr['id']]).transpose(),axis=1)\n",
    "comb_arr1 = np.append(comb_arr1,np.array([result1]).transpose(),axis=1)\n",
    "print(comb_arr1)\n",
    "\n",
    "np.savetxt(\"submission1.csv\", comb_arr1, delimiter=\",\", fmt = '% s', header='id,class', comments='')\n",
    "\n",
    "# with TF-IDF\n",
    "empty_array2=np.empty((1000,0),object)\n",
    "\n",
    "comb_arr2 = np.append(empty_array2,np.array([tstarr['id']]).transpose(),axis=1)\n",
    "comb_arr2 = np.append(comb_arr2,np.array([result2]).transpose(),axis=1)\n",
    "print(comb_arr2)\n",
    "\n",
    "np.savetxt(\"submission2.csv\", comb_arr2, delimiter=\",\", fmt = '% s', header='id,class', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
